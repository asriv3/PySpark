{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL StructType & StructField"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Spark SQL StructType & StructField classes are used to programmatically specify the schema to the DataFrame and creating complex columns like nested struct, array and map columns. StructType is a collection of StructFieldâ€™s that defines column name, column data type, boolean to specify if the field can be nullable or not and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Python & Spark Libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "conf = SparkConf().setAppName(\"Simple Masking App\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DataMasking on Nested Strcuture\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read file data in Spark RDD\n",
    "empRDD = sc.textFile(\"EmployData.csv\")\n",
    "#Remove Header and get only data\n",
    "empNoHeaderRDD = empRDD.mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)\n",
    "#Split columns with seprator comma(',')\n",
    "empRDD1 = empNoHeaderRDD.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each line is converted to a tuple.\n",
    "emp = empRDD1.map(lambda p: (p[0], p[1],p[2].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('James', 'james@gmail.com', 'Sr. Developer')\n",
      "('Smith', 'Smith@gmail.com', 'Project Lead')\n"
     ]
    }
   ],
   "source": [
    "for i in emp.take(10):print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schema is encoded in a string.\n",
    "schemaString = \"employee_name email job_profile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply new custom schema on RDD and create a dataframe\n",
    "newDF = spark.createDataFrame(emp,newSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- job_profile: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_name,StringType,true),StructField(email,StringType,true),StructField(job_profile,StringType,true)))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display Structs in dataframe schema\n",
    "newDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------+\n",
      "|employee_name|          email|  job_profile|\n",
      "+-------------+---------------+-------------+\n",
      "|        James|james@gmail.com|Sr. Developer|\n",
      "|        Smith|Smith@gmail.com| Project Lead|\n",
      "+-------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displace data from newDF dataframe\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduce new columns with struct type\n",
    "from pyspark.sql.functions import struct, col\n",
    "nestedDF_old = newDF.withColumn(\"Info\", struct(\n",
    "                 (col(\"email\") != \"null\").alias(\"email_exists\"),\n",
    "                 (col(\"job_profile\") != \"null\").alias(\"job_profile_exists\")\n",
    "             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------+------------+\n",
      "|employee_name|          email|  job_profile|        Info|\n",
      "+-------------+---------------+-------------+------------+\n",
      "|        James|james@gmail.com|Sr. Developer|[true, true]|\n",
      "|        Smith|Smith@gmail.com| Project Lead|[true, true]|\n",
      "+-------------+---------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Nested columns with boolean structs\n",
    "nestedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nested columns with string structs\n",
    "nestedDF = newDF.withColumn(\"Info\", struct(\n",
    "                 col(\"email\").alias(\"email_nested\"),\n",
    "                 col(\"job_profile\").alias(\"job_profile_nested\")\n",
    "             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- job_profile: string (nullable = true)\n",
      " |-- Info: struct (nullable = false)\n",
      " |    |-- email_nested: string (nullable = true)\n",
      " |    |-- job_profile_nested: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nestedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------+---------------+------------------+\n",
      "|employee_name|          email|  job_profile|   email_nested|job_profile_nested|\n",
      "+-------------+---------------+-------------+---------------+------------------+\n",
      "|        James|james@gmail.com|Sr. Developer|james@gmail.com|     Sr. Developer|\n",
      "|        Smith|Smith@gmail.com| Project Lead|Smith@gmail.com|      Project Lead|\n",
      "+-------------+---------------+-------------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nestedDF.select(\n",
    "                \"employee_name\",\"email\",\"job_profile\",\"info.email_nested\",\"info.job_profile_nested\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestedDF.select(\n",
    "                \"employee_name\",\"email\",\"job_profile\",\"info.email_nested\",\"info.job_profile_nested\"\n",
    ").createOrReplaceTempView(\"emp_nested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\"SELECT employee_name,email,job_profile,email_nested,job_profile_nested FROM emp_nested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------+---------------+------------------+\n",
      "|employee_name|          email|  job_profile|   email_nested|job_profile_nested|\n",
      "+-------------+---------------+-------------+---------------+------------------+\n",
      "|        James|james@gmail.com|Sr. Developer|james@gmail.com|     Sr. Developer|\n",
      "|        Smith|Smith@gmail.com| Project Lead|Smith@gmail.com|      Project Lead|\n",
      "+-------------+---------------+-------------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.maskColumn(colName)>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define Python Function to Mask column \n",
    "def maskColumn(colName):\n",
    "        return ('*****Masked Data*****')\n",
    "    \n",
    "spark.udf.register(\"maskColumn\", maskColumn, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\"SELECT employee_name,maskColumn(email_nested),maskColumn(job_profile_nested) FROM emp_nested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------+------------------------------+\n",
      "|employee_name|maskColumn(email_nested)|maskColumn(job_profile_nested)|\n",
      "+-------------+------------------------+------------------------------+\n",
      "|        James|    *****Masked Data*...|          *****Masked Data*...|\n",
      "|        Smith|    *****Masked Data*...|          *****Masked Data*...|\n",
      "+-------------+------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
